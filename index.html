<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Mengzhao chen&#39;s home page">

  <link href="./images/tlzdoc.css" rel="stylesheet" type="text/css">
  <title>MengzhaoChen's Homepage--陈锰钊的个人主页</title>
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body>
  <div id="layout-content" style="margin-top: 25px;">
    <table>
      <tbody>
        <tr>
          <td>
            <img width="200" src="./images/mengzhaochen_2.jpg">
          </td>
          <td width="670">
            <div id="toptitle">
              <h1>Mengzhao Chen &nbsp; 陈锰钊<a name="top"></a></h1>
            </div>
            <h3>First-year Ph.D. Student </h3>
            <p>
              Department of Computer Science, The University of Hong Kong
            </p>
            <p>
              <br> Email:
              <a href="mailto:chenmnz@connect.hku.hk">chenmnz@connect.hku.hk</a></a>
              <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              &nbsp;&nbsp;<a href="https://github.com/ChenMnZ" target="_blank">[Github]</a>
              &nbsp;&nbsp;<a href="https://scholar.google.com.hk/citations?user=dN7UtFkAAAAJ&hl=zh-CN"
                target="_blank">[Scholar]</a>
              <!--      &nbsp;&nbsp;<a href="#" target="_blank">[Ph.D. Thesis, in Chinese]</a>		-->
              <br><br>
            </p>
          </td>
        </tr>
        <tr></tr>
      </tbody>
    </table>
    <div id="layout-content" style="margin-top: 25px;">


      <!-- <h4>[<a style=" color:#9D849A;" href="#biography">Biography</a>] [<a style=" color:#9D849A;" href="#news">Latest
          News</a>] [<a style=" color:#9D849A;" href="#publications">Publications</a>]
        [<a style=" color:#9D849A;" href="#activities">Professional Activities</a>] [<a style=" color:#9D849A;"
          href="#awards">Major Awards</a>] [<a style=" color:#9D849A;" href="#statistics">Statistics</a>]</h4> -->


      <h2>About Me<a name="biography"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;" href="#top">[back
          top]</a></h2>
      <p style="text-indent:2em;">
        I am currently a first-year Ph.D. student in the 
        University of Hong Kong, fortunately supervised by <a href="http://luoping.me/" target="_blank">Prof. Ping Luo</a> at the 
        <a href="https://mmlab-hku.com/" target="_blank">MMLAB@HKU</a>.
        Earlier, I received the master and bachelor degree from <a href="https://mac.xmu.edu.cn/" target="_blank">MAC@XMU</a>, Xiamen University, and 
        advised by <a href="https://mac.xmu.edu.cn/rrji_en/" target="_blank">Prof. Rongrong Ji</a>.
      </p>
      <p style="text-indent:2em;">My research interests are to develop efficient models, including both vision and language models. Recently, I focus on develop 
        quantization algorithm for Large Language Models (LLMs). 
      </p>
      <p style="text-indent:2em;"> <b><font color="red">I am actively looking for academic collaboration, fell free to contact me if you are interested.</font></b>
      </p>



      <h2>Latest News<a name="news"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;" href="#top">[back
          top]</a></h2>
      <ul>
        <li> [2024.10]: Awarded the <a href="https://mp.weixin.qq.com/s?__biz=MjM5NzIwODcyMQ==&mid=2663377716&idx=1&sn=a2285a907c843cef465848a976b95b10&chksm=bc9398fe566a7630f88609a0cc41c589341a5a15b44bee7cd40bf6f1413de48a997f4b2e454f&mpshare=1&scene=2&srcid=11050oU1mDdQhJ5yaT0QlUpR&sharer_shareinfo=fb05f7a70f05bc4e617185163bc7ca12&sharer_shareinfo_first=301a63875b9ecf6c1398d9f11f9f163a#rd" target="_blank">中国电子学会-腾讯博士生科研激励计划</a>  <br>
          (10W RMB, <b><font color="red">17 PHD students in China</font></b>) </li>
        <li> [2024.09] Start the PHD journal at <a href="https://mmlab-hku.com/" target="_blank">HKU-MMLab</a></li>
        <li> [2024.01]: Two paper are accepted by ICLR 2024, including one <b><font color="red">Spotlight</font></b> </li>
        <li> [2023.10]: Awarded the National Scholarship  <br>
          (国家奖学金, <b><font color="red">Top 1% in Xiamen University</font></b>) </li>
        <li> [2023.07]: Two paper are accepted by ICCV 2023 </li>
        <li> [2023.07]: One paper is accepted by IJCV  </li>
        <li> [2022.11]: One paper is accepted as <b><font color="red">oral presentation</font></b> by AAAI 2023 </li>
      </ul>


      <h2>Publications<a name="publications"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <h3>Preprint</h3>
      <table class="pub_table">
        <tr>
          <td class="pub_td1"><img width="200" img src="./images/prefixquant.png" class="papericon"></td>
          <td class="pub_td2"><b>Mengzhao Chen</b>,  Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao<sup>✉</sup>, Ping Luo<sup>✉</sup>
            <br><b>PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs</b>
            <br>
            [<a href="https://arxiv.org/abs/2410.05265" target="_blank">arXiv</a>]
            [<a href="https://github.com/ChenMnZ/PrefixQuant" target="_blank">code</a> <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ChenMnZ/PrefixQuant" alt="GitHub stars" title="" />]
          </td>
        </tr>
      </table>
      <table class="pub_table">
        <tr>
          <td class="pub_td1"><img width="200" img src="./images/efficientqat.png" class="papericon"></td>
          <td class="pub_td2"><b>Mengzhao Chen</b>, Wenqi Shao<sup>✉</sup>, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo<sup>✉</sup>
            <br><b>EfficientQAT: Efficient Quantization-Aware Training for Large Language Models</b>
            <br>
            [<a href="https://arxiv.org/abs/2407.11062" target="_blank">arXiv</a>]
            [<a href="https://github.com/OpenGVLab/EfficientQAT" target="_blank">code</a><img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/OpenGvlab/EfficientQAT" alt="GitHub stars" title="" />]
            [<a href="https://zhuanlan.zhihu.com/p/710189686" target="_blank">中文介绍</a>]
          </td>
        </tr>
      </table>
      <h3>Journal</h3>
      <table class="pub_table">
        <tr>
          <td class="pub_td1"><img width="200" img src="./images/supervit.png" class="papericon"></td>
          <td class="pub_td2">Mingbao Lin<sup>*</sup>, <b>Mengzhao Chen</b><sup>*</sup>, Yuxin Zhang, Ke Li, Yunhang
            Shen, Chunhua Shen,
            Rongrong Ji, Liujuan Cao<sup>✉</sup>
            <br><b>Super Vision Transformer</b>
            <br>International Journal of Computer Vision (IJCV), 2023
            <br>
            [<a href="https://arxiv.org/abs/2205.11397" target="_blank">arXiv</a>]
            [<a href="https://github.com/lmbxmu/SuperViT" target="_blank">code</a>]
            (<sup>*</sup> Equal Contribution)	
          </td>
        </tr>
      </table>
      <h3>Conference</h3>
      <table class="pub_table">
        <tbody>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/omniquant.png" class="papericon"></td>
            <td class="pub_td2">Wenqi Shao<sup>*</sup>,<b>Mengzhao Chen</b><sup>*</sup>, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo<sup>✉</sup>
              <br><b>OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</b>
              <br>International Conference on Learning Representations (ICLR, Spotlight), 2024
              <br>
              [<a href="https://arxiv.org/abs/2308.13137" target="_blank">arXiv</a>]
              [<a href="https://github.com/OpenGVLab/OmniQuant" target="_blank">code</a>  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/OpenGvlab/OmniQuant" alt="GitHub stars" title="" />]
              [<a href="https://mp.weixin.qq.com/s/za2ptWT1_li99-YmjcXQAg" target="_blank">中文介绍</a>]
              (<sup>*</sup> Equal Contribution)	
            </td>
          </tr>
          <tr>
          <tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/diffrate.png" class="papericon"></td>
            <td class="pub_td2"><b>Mengzhao Chen</b>,Wenqi Shao<sup>✉</sup>, Peng Xu, Mingbao Lin, Kaipeng Zhang, Fei Chao, Rongrong Ji<sup>✉</sup>, Yu Qiao, Ping Luo
              <br><b>DiffRate : Differentiable Compression Rate for Efficient Vision Transformers</b>
              <br>International Conference on Computer Vision (ICCV), 2023
              <br>
              [<a href="https://arxiv.org/abs/2305.17997" target="_blank">arXiv</a>]
              [<a href="https://github.com/OpenGVLab/DiffRate" target="_blank">code</a> <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/OpenGvlab/DiffRate" alt="GitHub stars" title="" />]
              [<a href="https://mp.weixin.qq.com/s/N_dkYCQIapQOwGlO0BeClQ" target="_blank">中文介绍</a>]
              [<a href="https://mp.weixin.qq.com/s/4UIjykgbslIUxB--pcU4UQ" target="_blank">直播回放</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/smmix.png" class="papericon"></td>
            <td class="pub_td2"><b>Mengzhao Chen</b>, Mingbao Lin, Zhihang Lin, Yuxin Zhang, Fei Chao, Rongrong
              Ji<sup>✉</sup>
              <br><b>SMMix: Self-Motivated Image Mixing for Vision Transformers</b>
              <br>International Conference on Computer Vision (ICCV), 2023
              <br>
              [<a href="https://arxiv.org/abs/2212.12977" target="_blank">arXiv</a>]
              [<a href="https://github.com/ChenMnZ/SMMix" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/cfvit.png" class="papericon"></td>
            <td class="pub_td2"><b>Mengzhao Chen</b>, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, Rongrong
              Ji<sup>✉</sup>
              <br><b>CF-ViT: A General Coarse-to-Fine Method for Vision Transformer</b>
              <br> AAAI Conference on Artificial Intelligence (AAAI, Oral), 2023
              <br>
              [<a href="https://arxiv.org/abs/2203.03821" target="_blank">arXiv</a>]
              [<a href="https://github.com/ChenMnZ/CF-ViT" target="_blank">code</a>  <img style="border: 0px;padding: 0px;border-radius: 5px;" src="https://img.shields.io/github/stars/ChenMnZ/CF-ViT" alt="GitHub stars" title="" />]
              [<a href="https://mp.weixin.qq.com/s/J_wDSANS2DselnIUM5kR7w" target="_blank">中文介绍</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/besa.png" class="papericon"></td>
            <td class="pub_td2">Peng Xu, Wenqi Shao<sup>✉</sup>,<b>Mengzhao Chen</b>, Shitong Tang, Kaipeng Zhang, Peng Gao,  Fengwei An, Yu Qiao, Ping Luo<sup>✉</sup>
              <br><b>Besa: Pruning large language models with blockwise parameter-efficient sparsity allocation</b>
              <br>International Conference on Learning Representations (ICLR), 2024
              <br>
              [<a href="https://arxiv.org/abs/2402.16880" target="_blank">arXiv</a>]
              [<a href="https://github.com/OpenGVLab/LLMPrune-BESA" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/fdda.jpg" class="papericon"></td>
            <td class="pub_td2"> Yunshan Zhong, Mingbao Lin, <b>Mengzhao Chen</b>, Ke Li, Yunhang Shen, Fei Chao,
              Yongjian Wu, Rongrong Ji<sup>✉</sup>
              <br><b>Fine-grained Data Distribution Alignment for Post-Training Quantization</b>
              <br>European Conference on Computer Vision (ECCV), 2022
              <br>
              [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710070.pdf"
                target="_blank">pdf</a>|<a
                href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710070-supp.pdf"
                target="_blank">supp</a>]
              [<a href="http://arxiv.org/abs/2109.04186" target="_blank">arXiv</a>]
              [<a href="https://github.com/zysxmu/FDDA" target="_blank">code</a>]
            </td>
          </tr>

        </tbody>
      </table>


      <h2>Major Awards<a name="awards"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <ul>
        <li><a href="https://mp.weixin.qq.com/s?__biz=MjM5NzIwODcyMQ==&mid=2663377716&idx=1&sn=a2285a907c843cef465848a976b95b10&chksm=bc9398fe566a7630f88609a0cc41c589341a5a15b44bee7cd40bf6f1413de48a997f4b2e454f&mpshare=1&scene=2&srcid=11050oU1mDdQhJ5yaT0QlUpR&sharer_shareinfo=fb05f7a70f05bc4e617185163bc7ca12&sharer_shareinfo_first=301a63875b9ecf6c1398d9f11f9f163a#rd" target="_blank">中国电子学会-腾讯博士生科研激励计划</a>, 2024   <br>
          (10W RMB, <b><font color="red">17 PHD students in China</font></b>)</li>
        <li> Outstanding Graduate Student, Xiamen University, 2024 </li>
        <li> National Scholarship, Ministry of Education of China, 2023 
          <br>
          (国家奖学金, <b><font color="red">Top 1% in Xiamen University</font></b>)
        </li>
        <li> Outstanding Graduate Student, Xiamen University, 2021 </li>
      </ul>

      <h2>Academic Service<a name="awards"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <ul>
        <li> Reviewer of TPAMI, IJCV, TMLR</li>
        <li> Reviewer of NeruIPS, CVPR, ICLR, ECCV, CPAL, AISTATS</li>
      </ul>

      <h2>Statistics<a name="statistics"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <script type="text/javascript" id="clustrmaps"
        src="//clustrmaps.com/map_v2.js?d=bvtAWGDaBy6vsZgB58FPszWuQnUJmD1hMQJpOvE_hzQ&cl=ffffff&w=a"></script>

    </div>
  </div>
</body>

</html>