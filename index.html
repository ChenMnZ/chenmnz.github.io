<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
  <meta name="description" content="Mengzhao chen&#39;s home page">

  <link href="./images/tlzdoc.css" rel="stylesheet" type="text/css">
  <title>MengzhaoChen's Homepage--陈锰钊的个人主页</title>
  <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
</head>


<body>
  <div id="layout-content" style="margin-top: 25px;">
    <table>
      <tbody>
        <tr>
          <td>
            <img width="200" src="./images/mengzhaochen_2.jpg">
          </td>
          <td width="670">
            <div id="toptitle">
              <h1>Mengzhao Chen &nbsp; 陈锰钊<a name="top"></a></h1>
            </div>
            <!-- <h3>Ph.D. Student</h3> -->
            <p>
              <br> Email:
              <a href="mailto:cmzxmu@stu.xmu.edu.cn">cmzxmu@stu.xmu.edu.cn</a>
              <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              &nbsp;&nbsp;<a href="https://github.com/ChenMnZ" target="_blank">[Github]</a>
              &nbsp;&nbsp;<a href="https://scholar.google.com.hk/citations?user=dN7UtFkAAAAJ&hl=zh-CN"
                target="_blank">[Scholar]</a>
              <!--      &nbsp;&nbsp;<a href="#" target="_blank">[Ph.D. Thesis, in Chinese]</a>		-->
              <br><br>
            </p>
          </td>
        </tr>
        <tr></tr>
      </tbody>
    </table>
    <div id="layout-content" style="margin-top: 25px;">


      <!-- <h4>[<a style=" color:#9D849A;" href="#biography">Biography</a>] [<a style=" color:#9D849A;" href="#news">Latest
          News</a>] [<a style=" color:#9D849A;" href="#publications">Publications</a>]
        [<a style=" color:#9D849A;" href="#activities">Professional Activities</a>] [<a style=" color:#9D849A;"
          href="#awards">Major Awards</a>] [<a style=" color:#9D849A;" href="#statistics">Statistics</a>]</h4> -->


      <h2>About Me<a name="biography"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;" href="#top">[back
          top]</a></h2>
      <p style="text-indent:2em;">
        I am currently an incoming Ph.D. student in the 
        University of Hong Kong, fortunately supervised by <a href="http://luoping.me/" target="_blank">Prof. Ping Luo</a> at the 
        <a href="https://mmlab-hku.com/" target="_blank">MMLAB@HKU</a>.
        Earlier, I received the master and bachelor degree from <a href="https://mac.xmu.edu.cn/" target="_blank">MAC@XMU</a>, Xiamen University, and 
        advised by <a href="https://mac.xmu.edu.cn/rrji_en/" target="_blank">Prof. Rongrong Ji</a>.
      </p>
      <p style="text-indent:2em;">My research interests are to develop efficient models, including both vision and language models. Recently, I focus on develop 
        quantization algorithm for Large Language Models (LLMs). Fell free to contact me for any problem or collaboration.
      </p>



      <h2>Latest News<a name="news"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;" href="#top">[back
          top]</a></h2>
      <ul>
        <li> 1/2024: Two paper are accepted by ICLR 2024, including one Spotlight </li>
        <li> 7/2023: Two paper are accepted by ICCV 2023 </li>
        <li> 7/2023: One paper is accepted by IJCV  </li>
        <li> 11/2022: One paper is accepted as oral presentation by AAAI 2023 </li>
      </ul>


      <h2>Publications<a name="publications"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <h3>Preprint</h3>
      <table class="pub_table">
        <tr>
          <td class="pub_td1"><img width="200" img src="./images/efficientqat.png" class="papericon"></td>
          <td class="pub_td2"><b>Mengzhao Chen</b>, Wenqi Shao<sup>✉</sup>, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo<sup>✉</sup>
            <br><b>EfficientQAT: Efficient Quantization-Aware Training for Large Language Models</b>
            <br>
            [<a href="https://arxiv.org/abs/2407.11062" target="_blank">arXiv</a>]
            [<a href="https://github.com/OpenGVLab/EfficientQAT" target="_blank">code</a>]
            [<a href="https://zhuanlan.zhihu.com/p/710189686" target="_blank">中文介绍</a>]
          </td>
        </tr>
      </table>
      <h3>Journal</h3>
      <table class="pub_table">
        <tr>
          <td class="pub_td1"><img width="200" img src="./images/supervit.png" class="papericon"></td>
          <td class="pub_td2">Mingbao Lin<sup>*</sup>, <b>Mengzhao Chen</b><sup>*</sup>, Yuxin Zhang, Ke Li, Yunhang
            Shen, Chunhua Shen,
            Rongrong Ji, Liujuan Cao<sup>✉</sup>
            <br><b>Super Vision Transformer</b>
            <br>International Journal of Computer Vision (IJCV), 2023
            <br>
            [<a href="https://arxiv.org/abs/2205.11397" target="_blank">arXiv</a>]
            [<a href="https://github.com/lmbxmu/SuperViT" target="_blank">code</a>]
            (<sup>*</sup> Equal Contribution)	
          </td>
        </tr>
      </table>
      <h3>Conference</h3>
      <table class="pub_table">
        <tbody>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/omniquant.png" class="papericon"></td>
            <td class="pub_td2">Wenqi Shao<sup>*</sup>,<b>Mengzhao Chen</b><sup>*</sup>, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo<sup>✉</sup>
              <br><b>OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</b>
              <br>International Conference on Learning Representations (ICLR, Spotlight), 2024
              <br>
              [<a href="https://arxiv.org/abs/2308.13137" target="_blank">arXiv</a>]
              [<a href="https://github.com/OpenGVLab/OmniQuant" target="_blank">code (600+ stars)</a>]
              [<a href="https://mp.weixin.qq.com/s/za2ptWT1_li99-YmjcXQAg" target="_blank">中文介绍</a>]
              (<sup>*</sup> Equal Contribution)	
            </td>
          </tr>
          <tr>
          <tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/diffrate.png" class="papericon"></td>
            <td class="pub_td2"><b>Mengzhao Chen</b>,Wenqi Shao<sup>✉</sup>, Peng Xu, Mingbao Lin, Kaipeng Zhang, Fei Chao, Rongrong Ji<sup>✉</sup>, Yu Qiao, Ping Luo
              <br><b>DiffRate : Differentiable Compression Rate for Efficient Vision Transformers</b>
              <br>International Conference on Computer Vision (ICCV), 2023
              <br>
              [<a href="https://arxiv.org/abs/2305.17997" target="_blank">arXiv</a>]
              [<a href="https://github.com/OpenGVLab/DiffRate" target="_blank">code</a>]
              [<a href="https://mp.weixin.qq.com/s/N_dkYCQIapQOwGlO0BeClQ" target="_blank">中文介绍</a>]
              [<a href="https://mp.weixin.qq.com/s/4UIjykgbslIUxB--pcU4UQ" target="_blank">直播回放</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/smmix.png" class="papericon"></td>
            <td class="pub_td2"><b>Mengzhao Chen</b>, Mingbao Lin, Zhihang Lin, Yuxin Zhang, Fei Chao, Rongrong
              Ji<sup>✉</sup>
              <br><b>SMMix: Self-Motivated Image Mixing for Vision Transformers</b>
              <br>International Conference on Computer Vision (ICCV), 2023
              <br>
              [<a href="https://arxiv.org/abs/2212.12977" target="_blank">arXiv</a>]
              [<a href="https://github.com/ChenMnZ/SMMix" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/cfvit.png" class="papericon"></td>
            <td class="pub_td2"><b>Mengzhao Chen</b>, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, Rongrong
              Ji<sup>✉</sup>
              <br><b>CF-ViT: A General Coarse-to-Fine Method for Vision Transformer</b>
              <br> AAAI Conference on Artificial Intelligence (AAAI, Oral), 2023
              <br>
              [<a href="https://arxiv.org/abs/2203.03821" target="_blank">arXiv</a>]
              [<a href="https://github.com/ChenMnZ/CF-ViT" target="_blank">code (100+ stars)</a>]
              [<a href="https://mp.weixin.qq.com/s/J_wDSANS2DselnIUM5kR7w" target="_blank">中文介绍</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/besa.png" class="papericon"></td>
            <td class="pub_td2">Peng Xu, Wenqi Shao<sup>✉</sup>,<b>Mengzhao Chen</b>, Shitong Tang, Kaipeng Zhang, Peng Gao,  Fengwei An, Yu Qiao, Ping Luo<sup>✉</sup>
              <br><b>Besa: Pruning large language models with blockwise parameter-efficient sparsity allocation</b>
              <br>International Conference on Learning Representations (ICLR), 2024
              <br>
              [<a href="https://arxiv.org/abs/2402.16880" target="_blank">arXiv</a>]
              [<a href="https://github.com/OpenGVLab/LLMPrune-BESA" target="_blank">code</a>]
            </td>
          </tr>
          <tr>
            <td class="pub_td1"><img width="200" img src="./images/fdda.jpg" class="papericon"></td>
            <td class="pub_td2"> Yunshan Zhong, Mingbao Lin, <b>Mengzhao Chen</b>, Ke Li, Yunhang Shen, Fei Chao,
              Yongjian Wu, Rongrong Ji<sup>✉</sup>
              <br><b>Fine-grained Data Distribution Alignment for Post-Training Quantization</b>
              <br>European Conference on Computer Vision (ECCV), 2022
              <br>
              [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710070.pdf"
                target="_blank">pdf</a>|<a
                href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710070-supp.pdf"
                target="_blank">supp</a>]
              [<a href="http://arxiv.org/abs/2109.04186" target="_blank">arXiv</a>]
              [<a href="https://github.com/zysxmu/FDDA" target="_blank">code</a>]
            </td>
          </tr>

        </tbody>
      </table>


      <h2>Major Awards<a name="awards"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <ul>
        <li> Outstanding Graduate Student, Xiamen University, 2024 </li>
        <li> National Scholarship, Ministry of Education of China, 2023 </li>
        <li> Outstanding Graduate Student, Xiamen University, 2021 </li>
      </ul>

      <h2>Academic Service<a name="awards"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <ul>
        <li> Reviewer of TPAMI, IJCV, TMLR</li>
        <li> Reviewer of NeruIPS, CVPR, ECCV, CPAL</li>
      </ul>

      <h2>Statistics<a name="statistics"></a>&nbsp;&nbsp;&nbsp;<a style=" color:#9D849A; font-size:15px;"
          href="#top">[back top]</a></h2>
      <script type="text/javascript" id="clustrmaps"
        src="//clustrmaps.com/map_v2.js?d=bvtAWGDaBy6vsZgB58FPszWuQnUJmD1hMQJpOvE_hzQ&cl=ffffff&w=a"></script>

    </div>
  </div>
</body>

</html>